{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook covers the evaluation of the trained machine learning models using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Results\n",
    "\n",
    "We will load the results of the different models and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for model results\n",
    "# Replace these dictionaries with the actual results from your model training\n",
    "results_rf = {'accuracy': 0.85, 'precision': 0.86, 'recall': 0.84, 'f1': 0.85, 'roc_auc': 0.90, 'confusion_matrix': np.array([[85, 15], [10, 90]])}\n",
    "results_gb = {'accuracy': 0.83, 'precision': 0.84, 'recall': 0.82, 'f1': 0.83, 'roc_auc': 0.88, 'confusion_matrix': np.array([[83, 17], [12, 88]])}\n",
    "results_pls = {'accuracy': 0.80, 'precision': 0.81, 'recall': 0.79, 'f1': 0.80, 'roc_auc': 0.85, 'confusion_matrix': np.array([[80, 20], [15, 85]])}\n",
    "results_gpr = {'accuracy': 0.82, 'precision': 0.83, 'recall': 0.81, 'f1': 0.82, 'roc_auc': 0.87, 'confusion_matrix': np.array([[82, 18], [13, 87]])}\n",
    "results_nn = {'accuracy': 0.84, 'precision': 0.85, 'recall': 0.83, 'f1': 0.84, 'roc_auc': 0.89, 'confusion_matrix': np.array([[84, 16], [11, 89]])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "We will evaluate the models using various metrics and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Failure', 'Failure'], yticklabels=['No Failure', 'Failure'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(results_rf['confusion_matrix'], title='Random Forest Confusion Matrix')\n",
    "plot_confusion_matrix(results_gb['confusion_matrix'], title='Gradient Boosting Confusion Matrix')\n",
    "plot_confusion_matrix(results_pls['confusion_matrix'], title='PLS Regression Confusion Matrix')\n",
    "plot_confusion_matrix(results_gpr['confusion_matrix'], title='Gaussian Process Regression Confusion Matrix')\n",
    "plot_confusion_matrix(results_nn['confusion_matrix'], title='Neural Networks Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare performance\n",
    "performance = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'PLS Regression', 'Gaussian Process Regression', 'Neural Networks'],\n",
    "    'Accuracy': [results_rf['accuracy'], results_gb['accuracy'], results_pls['accuracy'], results_gpr['accuracy'], results_nn['accuracy']],\n",
    "    'Precision': [results_rf['precision'], results_gb['precision'], results_pls['precision'], results_gpr['precision'], results_nn['precision']],\n",
    "    'Recall': [results_rf['recall'], results_gb['recall'], results_pls['recall'], results_gpr['recall'], results_nn['recall']],\n",
    "    'F1 Score': [results_rf['f1'], results_gb['f1'], results_pls['f1'], results_gpr['f1'], results_nn['f1']],\n",
    "    'ROC AUC': [results_rf['roc_auc'], results_gb['roc_auc'], results_pls['roc_auc'], results_gpr['roc_auc'], results_nn['roc_auc']]\n",
    "})\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance\n",
    "performance.set_index('Model').plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "We will plot the ROC curves for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder ROC curves\n",
    "# Replace these lines with the actual predictions of your models\n",
    "y_pred_rf = np.random.rand(len(y_test))\n",
    "y_pred_gb = np.random.rand(len(y_test))\n",
    "y_pred_pls = np.random.rand(len(y_test))\n",
    "y_pred_gpr = np.random.rand(len(y_test))\n",
    "y_pred_nn = np.random.rand(len(y_test))\n",
    "\n",
    "# Compute ROC curve and AUC for each model\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_pred_gb)\n",
    "fpr_pls, tpr_pls, _ = roc_curve(y_test, y_pred_pls)\n",
    "fpr_gpr, tpr_gpr, _ = roc_curve(y_test, y_pred_gpr)\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_pred_nn)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = {:.2f})'.format(results_rf['roc_auc']))\n",
    "plt.plot(fpr_gb, tpr_gb, label='Gradient Boosting (AUC = {:.2f})'.format(results_gb['roc_auc']))\n",
    "plt.plot(fpr_pls, tpr_pls, label='PLS Regression (AUC = {:.2f})'.format(results_pls['roc_auc']))\n",
    "plt.plot(fpr_gpr, tpr_gpr, label='Gaussian Process Regression (AUC = {:.2f})'.format(results_gpr['roc_auc']))\n",
    "plt.plot(fpr_nn, tpr_nn, label='Neural Networks (AUC = {:.2f})'.format(results_nn['roc_auc']))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have evaluated the performance of various machine learning models using different metrics and visualizations. The results help in comparing the models and selecting the best one for predictive maintenance."
   ]
  }
 ]
}



