{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook covers the evaluation of the trained machine learning models using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluate import load_preprocessed_data, load_model, evaluate_and_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "X_test, y_test = load_preprocessed_data('../data/selected_features_data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "model_names = ['random_forest_model.pkl', 'gradient_boosting_model.pkl', 'pls_regression_model.pkl', 'neural_network_model.pkl']\n",
    "models = {name.split('_model.pkl')[0].replace('_', ' ').title(): load_model(f'../models/{name}') for name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models and plot results\n",
    "results = evaluate_and_plot(models, X_test, y_test)\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation metrics\n",
    "results_df.to_csv('../results/evaluation_metrics.csv', index=True)\n",
    "print('Model evaluation completed. Results saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have evaluated the performance of various machine learning models using different metrics and visualizations. The results help in comparing the models and selecting the best one for predictive maintenance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
